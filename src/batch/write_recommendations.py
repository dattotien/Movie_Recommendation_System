import os

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.ml.recommendation import ALSModel

from utils import cassandra_connector as db

LOCAL_MODEL_PATH = os.environ.get("ALS_MODEL_LOCAL_PATH", "file:///app/src/batch/als_model_32m")
HDFS_MODEL_PATH = os.environ.get("ALS_MODEL_HDFS_PATH", "hdfs://namenode:9000/user/hadoop/als_model")
MODEL_LOAD_STRATEGY = os.environ.get("ALS_MODEL_LOAD_STRATEGY", "local-first").lower()  # local-first|hdfs-first|hdfs-only
AUTO_PUSH_MODEL_TO_HDFS = os.environ.get("ALS_MODEL_AUTO_PUSH", "true").lower() == "true"
ITEM_FACTORS_EXPORT_PATH = os.environ.get("ALS_ITEM_FACTORS_PATH", "hdfs://namenode:9000/user/hadoop/als_item_factors")
AUX_EXPORT_ITEM_FACTORS = os.environ.get("ALS_EXPORT_ITEM_FACTORS", "true").lower() == "true"
TOP_K = int(os.environ.get("ALS_TOP_K", "30"))


def load_model(spark: SparkSession) -> ALSModel:
    """
    T·∫£i ALS model theo chi·∫øn l∆∞·ª£c ∆∞u ti√™n:
    - local-first (default): th·ª≠ load model local, n·∫øu th·∫•t b·∫°i th√¨ th·ª≠ HDFS
    - hdfs-first          : ∆∞u ti√™n load t·ª´ HDFS
    - hdfs-only           : ch·ªâ load t·ª´ HDFS

    N·∫øu load t·ª´ local v√† AUTO_PUSH_MODEL_TO_HDFS=True th√¨ s·∫Ω t·ª± ƒë·ªông copy model l√™n HDFS.
    """
    strategies = {
        "local-first": ["local", "hdfs"],
        "hdfs-first": ["hdfs", "local"],
        "hdfs-only": ["hdfs"],
    }
    order = strategies.get(MODEL_LOAD_STRATEGY, ["local", "hdfs"])
    last_error = None

    for source in order:
        path = LOCAL_MODEL_PATH if source == "local" else HDFS_MODEL_PATH
        try:
            print(f"üîÑ ƒêang t·∫£i model ALS t·ª´ ({source.upper()}): {path}")
            model = ALSModel.load(path)
            print("‚úÖ T·∫£i model th√†nh c√¥ng.")

            if source == "local" and AUTO_PUSH_MODEL_TO_HDFS:
                try:
                    print(f"üöÄ Mirror model local l√™n HDFS: {HDFS_MODEL_PATH}")
                    model.write().overwrite().save(HDFS_MODEL_PATH)
                    print("‚úÖ ƒê√£ c·∫≠p nh·∫≠t model tr√™n HDFS.")
                except Exception as push_err:
                    print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ push model l√™n HDFS: {push_err}")

            if AUX_EXPORT_ITEM_FACTORS and ITEM_FACTORS_EXPORT_PATH:
                try:
                    print(f"üóÇ  ƒêang export itemFactors t·ªõi {ITEM_FACTORS_EXPORT_PATH} ...")
                    model.itemFactors.write.mode("overwrite").parquet(ITEM_FACTORS_EXPORT_PATH)
                    print("‚úÖ Export itemFactors th√†nh c√¥ng.")
                except Exception as export_err:
                    print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ export itemFactors: {export_err}")
            return model
        except Exception as err:
            print(f"‚ö†Ô∏è Kh√¥ng load ƒë∆∞·ª£c model t·ª´ {path}: {err}")
            last_error = err

    raise RuntimeError(
        f"Kh√¥ng th·ªÉ load model ALS. Chi·∫øn l∆∞·ª£c ƒë√£ th·ª≠: {order}. "
        f"Ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n LOCAL_MODEL_PATH/HDFS_MODEL_PATH. L·ªói cu·ªëi: {last_error}"
    )


def main():
    spark = SparkSession.builder.appName("WriteBatchRecs").getOrCreate()
    spark.sparkContext.setLogLevel("WARN")

    model = load_model(spark)

    print(f"T√≠nh recommendForAllUsers({TOP_K}) ...")
    recs_df = model.recommendForAllUsers(TOP_K) \
        .select(col("userId"),
                col("recommendations.movieId").alias("movies"))

    db.create_keyspace_and_table()
    session = db.get_cassandra_session()
    if session is None:
        raise RuntimeError("Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c Cassandra")

    total = 0
    for row in recs_df.toLocalIterator():
        db.write_recs(session, str(row.userId), row.movies)
        total += 1
        if total % 1000 == 0:
            print(f"ƒê√£ ghi {total} users...")

    print(f"Ho√†n t·∫•t, ƒë√£ ghi {total} users.")
    spark.stop()


if __name__ == "__main__":
    main()