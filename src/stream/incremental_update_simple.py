"""
Incremental Learning - Version ƒê∆°n Gi·∫£n
T√≠ch h·ª£p v√†o process_stream.py ƒë·ªÉ c·∫≠p nh·∫≠t model ƒë·ªãnh k·ª≥
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.types import IntegerType, FloatType
from pyspark.ml.recommendation import ALS, ALSModel

# C·∫•u h√¨nh
HDFS_MODEL_PATH = "hdfs://namenode:9000/user/hadoop/als_model"
HDFS_ORIGINAL_RATINGS = "hdfs://namenode:9000/user/hadoop/movielens/32M/ratings.csv"
HDFS_ACCUMULATED_RATINGS = "hdfs://namenode:9000/user/hadoop/accumulated_new_ratings"

# Ng∆∞·ª°ng ƒë·ªÉ trigger incremental update
MIN_RATINGS_FOR_UPDATE = 5000  # C·∫ßn √≠t nh·∫•t 5000 ratings m·ªõi
UPDATE_INTERVAL_BATCHES = 50  # C·∫≠p nh·∫≠t m·ªói 50 batches

def accumulate_new_ratings(spark, new_ratings_df, batch_id):
    """
    T√≠ch l≈©y ratings m·ªõi v√†o HDFS
    """
    try:
        # ƒê·ªçc ratings ƒë√£ t√≠ch l≈©y (n·∫øu c√≥)
        try:
            accumulated = spark.read.csv(
                HDFS_ACCUMULATED_RATINGS,
                header=True,
                inferSchema=True
            )
        except:
            accumulated = None
        
        # G·ªôp v·ªõi ratings m·ªõi
        if accumulated:
            all_new = accumulated.union(new_ratings_df).distinct()
        else:
            all_new = new_ratings_df
        
        # L∆∞u l·∫°i
        all_new.write.mode("overwrite").option("header", "true").csv(HDFS_ACCUMULATED_RATINGS)
        
        count = all_new.count()
        print(f"  ‚Üí ƒê√£ t√≠ch l≈©y {count} ratings m·ªõi (t·ªïng)")
        
        return count
    except Exception as e:
        print(f"  ‚ö†Ô∏è L·ªói khi t√≠ch l≈©y ratings: {e}")
        return 0

def incremental_update_model(spark, accumulated_ratings_df):
    """
    C·∫≠p nh·∫≠t model v·ªõi ratings m·ªõi (Partial Retrain)
    """
    print("\n=== B·∫ÆT ƒê·∫¶U INCREMENTAL UPDATE ===")
    
    try:
        # 1. Load model hi·ªán t·∫°i
        print("1. ƒêang load model hi·ªán t·∫°i...")
        model = ALSModel.load(HDFS_MODEL_PATH)
        rank = model.rank
        print(f"   ‚úÖ Model rank={rank}")
        
        # 2. Load ratings g·ªëc
        print("2. ƒêang load ratings g·ªëc...")
        original_ratings = spark.read.csv(
            HDFS_ORIGINAL_RATINGS,
            header=True,
            inferSchema=True
        ).select(
            col("userId").cast(IntegerType()),
            col("movieId").cast(IntegerType()),
            col("rating").cast(FloatType())
        )
        print(f"   ‚úÖ {original_ratings.count()} ratings g·ªëc")
        
        # 3. G·ªôp ratings (c≈© + m·ªõi)
        print("3. ƒêang g·ªôp ratings...")
        all_ratings = original_ratings.union(accumulated_ratings_df).distinct()
        total_count = all_ratings.count()
        print(f"   ‚úÖ T·ªïng: {total_count} ratings")
        
        # 4. Retrain model (v·ªõi √≠t iterations h∆°n - incremental)
        print("4. ƒêang retrain model (incremental)...")
        als = ALS(
            userCol="userId",
            itemCol="movieId",
            ratingCol="rating",
            coldStartStrategy="drop",
            nonnegative=True,
            rank=rank,  # Gi·ªØ nguy√™n rank
            maxIter=3,  # √çt iterations h∆°n (nhanh h∆°n)
            regParam=0.1,
            numUserBlocks=50,
            numItemBlocks=50
        )
        
        updated_model = als.fit(all_ratings)
        print("   ‚úÖ Retrain ho√†n t·∫•t")
        
        # 5. L∆∞u model m·ªõi
        print("5. ƒêang l∆∞u model m·ªõi...")
        updated_model.write().overwrite().save(HDFS_MODEL_PATH)
        print("   ‚úÖ ƒê√£ l∆∞u model m·ªõi")
        
        # 6. X√≥a ratings ƒë√£ t√≠ch l≈©y (ƒë√£ d√πng xong)
        print("6. ƒêang x√≥a ratings ƒë√£ t√≠ch l≈©y...")
        # (C√≥ th·ªÉ gi·ªØ l·∫°i ƒë·ªÉ backup)
        print("   ‚úÖ Ho√†n t·∫•t")
        
        print("=== INCREMENTAL UPDATE TH√ÄNH C√îNG ===\n")
        
        return updated_model
        
    except Exception as e:
        print(f"‚ùå L·ªói khi c·∫≠p nh·∫≠t model: {e}")
        import traceback
        traceback.print_exc()
        return None

def check_and_update_model(spark, new_ratings_df, batch_id):
    """
    Ki·ªÉm tra v√† c·∫≠p nh·∫≠t model n·∫øu ƒë·ªß ƒëi·ªÅu ki·ªán
    """
    # Ch·ªâ ki·ªÉm tra m·ªói UPDATE_INTERVAL_BATCHES batches
    if batch_id % UPDATE_INTERVAL_BATCHES != 0:
        return None
    
    print(f"\nüîç Batch {batch_id}: Ki·ªÉm tra incremental update...")
    
    # 1. T√≠ch l≈©y ratings m·ªõi
    total_accumulated = accumulate_new_ratings(spark, new_ratings_df, batch_id)
    
    # 2. Ki·ªÉm tra ƒëi·ªÅu ki·ªán
    if total_accumulated >= MIN_RATINGS_FOR_UPDATE:
        print(f"‚úÖ ƒê·ªß {total_accumulated} ratings m·ªõi (>= {MIN_RATINGS_FOR_UPDATE})")
        
        # 3. ƒê·ªçc ratings ƒë√£ t√≠ch l≈©y
        accumulated_ratings = spark.read.csv(
            HDFS_ACCUMULATED_RATINGS,
            header=True,
            inferSchema=True
        ).select(
            col("userId").cast(IntegerType()),
            col("movieId").cast(IntegerType()),
            col("rating").cast(FloatType())
        )
        
        # 4. C·∫≠p nh·∫≠t model
        updated_model = incremental_update_model(spark, accumulated_ratings)
        
        if updated_model:
            # Reload model trong global cache (n·∫øu c·∫ßn)
            # global global_als_model
            # global_als_model = updated_model
            print("‚úÖ Model ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t! Stream layer s·∫Ω d√πng model m·ªõi ·ªü batch ti·∫øp theo.")
        
        return updated_model
    else:
        print(f"‚è≥ Ch∆∞a ƒë·ªß ratings ({total_accumulated} < {MIN_RATINGS_FOR_UPDATE})")
        return None



