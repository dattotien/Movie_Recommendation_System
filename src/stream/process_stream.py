from pyspark.sql import SparkSession
from pyspark.sql.functions import split, col
from pyspark.sql.types import IntegerType, FloatType
from pyspark.ml.recommendation import ALSModel
import time
import sys
from collections import defaultdict

# --- PH·∫¶N 1: THAY TH·∫æ MOCK B·∫∞NG IMPORT TH·∫¨T ---

# 1.1. Import th∆∞ vi·ªán c·ªßa Ng∆∞·ªùi 3 (Cassandra Connector)
try:
    # PYTHONPATH ƒë√£ ƒë∆∞·ª£c set trong docker-compose.yml
    import utils.cassandra_connector as db_connector
    print("‚úÖ ƒê√£ import Cassandra_connector th√†nh c√¥ng!")
except ModuleNotFoundError:
    print("‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file 'utils/Cassandra_connector.py'.")
    print("üëâ ƒê·∫£m b·∫£o Ng∆∞·ªùi 3 ƒë√£ ho√†n th√†nh Task 3.2.")
    sys.exit(1)


# 1.2. Bi·∫øn Global cho Model ALS (Task c·ªßa Ng∆∞·ªùi 2)
# Bi·∫øn n√†y s·∫Ω gi·ªØ model ALS ƒë√£ hu·∫•n luy·ªán ƒë·ªÉ kh√¥ng ph·∫£i t·∫£i l·∫°i m·ªói batch
global_als_model = None

HDFS_MODEL_PATH = "hdfs://namenode:9000/user/hadoop/als_model" 

def get_real_model():
    """
    H√†m n√†y t·∫£i model ALS t·ª´ HDFS (n·∫øu ch∆∞a t·∫£i) v√† l∆∞u v√†o bi·∫øn global.
    Vi·ªác n√†y ƒë·∫£m b·∫£o ch√∫ng ta ch·ªâ t·∫£i model 1 L·∫¶N.
    """
    global global_als_model
    if global_als_model is None:
        print(f"--- [REAL MODEL] L·∫ßn ƒë·∫ßu, ƒëang t·∫£i m√¥ h√¨nh ALS t·ª´: {HDFS_MODEL_PATH} ---")
        try:
            # T·∫£i model ALS m√† Ng∆∞·ªùi 2 ƒë√£ hu·∫•n luy·ªán
            global_als_model = ALSModel.load(HDFS_MODEL_PATH)
            print("‚úÖ T·∫£i m√¥ h√¨nh ALS th√†nh c√¥ng!")
        except Exception as e:
            print(f"‚ùå L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng th·ªÉ t·∫£i model ALS t·ª´ HDFS.")
            print(f"üëâ H√£y ch·∫Øc ch·∫Øn Ng∆∞·ªùi 2 (H√† Anh) ƒë√£ ch·∫°y 'train_model.py' th√†nh c√¥ng.")
            print(f"L·ªói chi ti·∫øt: {e}")
            sys.exit(1) # D·ª´ng script n·∫øu kh√¥ng t·∫£i ƒë∆∞·ª£c model
            
    return global_als_model

# --- PH·∫¶N 2: H√ÄM X·ª¨ L√ù STREAM (foreachBatch) ƒê√É N√ÇNG C·∫§P ---

def process_batch(batch_df, batch_id):
    """
    H√†m n√†y ƒë∆∞·ª£c g·ªçi m·ªói khi Spark c√≥ m·ªôt "batch" (l√¥) d·ªØ li·ªáu m·ªõi t·ª´ Kafka.
    """
    print(f"\nƒêang x·ª≠ l√Ω Batch ID: {batch_id}")
    
    batch_count = batch_df.count()
    if batch_count > 0:
        print(f"Batch {batch_id} c√≥ {batch_count} ratings m·ªõi.")
        
        # 1. Thu th·∫≠p th√¥ng tin ratings m·ªõi: l∆∞u nh·ªØng phim user ƒë√£ rate th·∫•p (<= 2.0)
        low_rated_movies = defaultdict(set)  # {userId: {movieId1, movieId2, ...}}
        ratings_data = batch_df.select(
            col("userId").cast(IntegerType()).alias("userId"),
            col("movieId").cast(IntegerType()).alias("movieId"),
            col("rating").cast(FloatType()).alias("rating")
        ).collect()
        
        for rating_row in ratings_data:
            user_id = rating_row['userId']
            movie_id = rating_row['movieId']
            rating_value = rating_row['rating']
            if rating_value <= 2.0:  # Ng∆∞·ª°ng: rate <= 2.0 l√† "th·∫•p"
                low_rated_movies[user_id].add(movie_id)
                print(f"  ‚Üí User {user_id} ƒë√£ rate th·∫•p phim {movie_id} (rating={rating_value})")
        
        # 2. L·∫•y danh s√°ch user
        distinct_users_df = batch_df.select(col("userId").cast(IntegerType())).distinct()
        
        # 3. T·∫£i model ALS (t·ª´ cache global)
        model = get_real_model()
        if model is None:
            print("Model ALS ch∆∞a ƒë∆∞·ª£c t·∫£i. B·ªè qua batch.")
            return

        # 4. T√≠nh to√°n Top 10 g·ª£i √Ω
        print(f"--- [REAL MODEL] ƒêang t√≠nh to√°n Top 10 cho {distinct_users_df.count()} user... ---")
        recs_df = model.recommendForUserSubset(distinct_users_df, 60)
        
        # 5. Thu th·∫≠p k·∫øt qu·∫£
        results = recs_df.select("userId", col("recommendations.movieId").alias("movies_list")).collect()
        
        if results:
            print(f"Batch {batch_id} c√≥ {len(results)} users ƒë·ªÉ c·∫≠p nh·∫≠t v√†o Cassandra:")
            
            # --- PH·∫¶N S·ª¨A L·ªñI QUAN TR·ªåNG ---
            
            # 6. L·∫§Y SESSION 1 L·∫¶N DUY NH·∫§T (·ªü ngo√†i v√≤ng l·∫∑p)
            #    S·ª≠ d·ª•ng h√†m get_cassandra_session() m√† Ng∆∞·ªùi 3 ƒë√£ vi·∫øt
            db_connector.create_keyspace_and_table()
            session = db_connector.get_cassandra_session()
            
            if session is None:
                print("‚ùå Kh√¥ng th·ªÉ l·∫•y session Cassandra. B·ªè qua ghi v√†o DB.")
                return

            # 7. Ghi t·ª´ng user v√†o Cassandra (TRUY·ªÄN session v√†o)
            for row in results:
                user_id = row['userId']
                recs_list = row['movies_list']
                
                # L·ªåC: Lo·∫°i b·ªè nh·ªØng phim user ƒë√£ rate th·∫•p trong batch n√†y
                if user_id in low_rated_movies:
                    excluded_movies = low_rated_movies[user_id]
                    recs_list_filtered = [movie_id for movie_id in recs_list if movie_id not in excluded_movies]
                    if len(recs_list_filtered) < len(recs_list):
                        print(f"  ‚Üí User {user_id}: ƒê√£ lo·∫°i b·ªè {len(recs_list) - len(recs_list_filtered)} phim rate th·∫•p kh·ªèi top 10")
                    recs_list = recs_list_filtered
                
                # S·ª¨A 1: Truy·ªÅn session v√†o
                # ƒê√¢y ch√≠nh l√† b·∫£n T·ªêI ∆ØU
                db_connector.write_recs(session, str(user_id), recs_list)
            
            # L∆∞u √Ω: Kh√¥ng shutdown session ·ªü ƒë√¢y, v√¨ n√≥ l√† global
            print(f"--- [REAL DB] ƒê√£ ghi xong {len(results)} users v√†o Cassandra (ƒê√É T·ªêI ∆ØU) ---")
            
        else:
            print(f"Batch {batch_id} kh√¥ng c√≥ k·∫øt qu·∫£ (l·ªói t√≠nh to√°n?).")
    else:
        print(f"Batch {batch_id} kh√¥ng c√≥ data m·ªõi.")

# --- PH·∫¶N 3: H√ÄM MAIN (Gi·ªØ nguy√™n) ---

def main():
    print("Kh·ªüi ƒë·ªông job Spark Streaming (L·ªöP SPEED - PHI√äN B·∫¢N TH·∫¨T)...")
    
    spark = SparkSession.builder \
        .appName("SpeedLayerProcessor_REAL") \
        .getOrCreate()
    spark.sparkContext.setLogLevel("WARN") # Gi·∫£m log r√°c

    # 1. ƒê·ªçc (Read) t·ª´ Kafka
    df = spark \
      .readStream \
      .format("kafka") \
      .option("kafka.bootstrap.servers", "kafka:9092") \
      .option("subscribe", "new_ratings") \
      .option("startingOffsets", "latest") \
      .load()
    
    print("ƒê√£ k·∫øt n·ªëi Kafka, ƒëang l·∫Øng nghe topic 'new_ratings'...")

    # 2. X·ª≠ l√Ω (Parse) message "userId,movieId,rating"
    ratings_df = df.selectExpr("CAST(value AS STRING)") \
        .select(
            split(col("value"), ",")[0].cast(IntegerType()).alias("userId"),
            split(col("value"), ",")[1].cast(IntegerType()).alias("movieId"),
            split(col("value"), ",")[2].cast(FloatType()).alias("rating")
        )

    # 3. Ghi (Write) b·∫±ng h√†m 'foreachBatch'
    query = ratings_df \
      .writeStream \
      .trigger(processingTime='15 seconds') \
      .outputMode("update") \
      .foreachBatch(process_batch) \
      .start()
      
    print("ƒê√£ kh·ªüi ƒë·ªông query, ƒëang ch·ªù data...")
    
    query.awaitTermination()

if __name__ == "__main__":
    main()